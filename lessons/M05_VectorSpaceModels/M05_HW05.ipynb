{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW05\n",
    "\n",
    "```yaml\n",
    "course:   DS 5001 \n",
    "module:   Module 05 HW\n",
    "topic:    BOW and TFIDF\n",
    "author:   Ryan Lipps\n",
    "date:     17 February 2024\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Change this to match the location of your data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env.ini\")\n",
    "data_home = config['DEFAULT']['data_home'] \n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "data_prefix = 'austen-melville'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ryanlipps/Documents/MSDS/DS5001/data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num']\n",
    "bags = dict(\n",
    "    SENTS = OHCO[:4],\n",
    "    PARAS = OHCO[:3],\n",
    "    CHAPS = OHCO[:2],\n",
    "    BOOKS = OHCO[:1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bag = 'CHAPS'\n",
    "# bag = 'BOOKS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LIB and CORPUS tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIB = pd.read_csv(f\"{output_dir}/{data_prefix}-LIB.csv\").set_index('book_id')\n",
    "CORPUS = pd.read_csv(f'{output_dir}/{data_prefix}-CORPUS.csv').set_index('book_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>chap_regex</th>\n",
       "      <th>book_len</th>\n",
       "      <th>n_chaps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>PERSUASION</td>\n",
       "      <td>^Chapter\\s+\\d+$</td>\n",
       "      <td>83624</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>NORTHANGER ABBEY</td>\n",
       "      <td>^CHAPTER\\s+\\d+$</td>\n",
       "      <td>77601</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>MANSFIELD PARK</td>\n",
       "      <td>^CHAPTER\\s+[IVXLCM]+$</td>\n",
       "      <td>160378</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>EMMA</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+\\s*$</td>\n",
       "      <td>160926</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>SENSE AND SENSIBILITY</td>\n",
       "      <td>^CHAPTER\\s+\\d+$</td>\n",
       "      <td>119873</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>LADY SUSAN</td>\n",
       "      <td>^\\s*[IVXLCM]+\\s*$</td>\n",
       "      <td>23116</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>LOVE AND FREINDSHIP SIC</td>\n",
       "      <td>^\\s*LETTER .* to .*$</td>\n",
       "      <td>33265</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "      <td>PRIDE AND PREJUDICE</td>\n",
       "      <td>^Chapter\\s+\\d+$</td>\n",
       "      <td>122126</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>TYPEE A ROMANCE OF THE SOUTH SEAS</td>\n",
       "      <td>^CHAPTER</td>\n",
       "      <td>108021</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>MOBY DICK OR THE WHALE</td>\n",
       "      <td>^(?:ETYMOLOGY|EXTRACTS|CHAPTER)</td>\n",
       "      <td>215504</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>OMOO ADVENTURES IN THE SOUTH SEAS</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$</td>\n",
       "      <td>102352</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8118</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>REDBURN HIS FIRST VOYAGE BEING THE SAILOR BOY ...</td>\n",
       "      <td>^\\s*[IVXLCM]+\\. .*$</td>\n",
       "      <td>119243</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10712</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>WHITE JACKET OR THE WORLD ON A MAN OF WAR</td>\n",
       "      <td>^CHAPTER\\s+[IVXLCM]+\\.\\s*$</td>\n",
       "      <td>143310</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13720</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>MARDI AND A VOYAGE THITHER VOL I</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+\\s*$</td>\n",
       "      <td>96878</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13721</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>MARDI AND A VOYAGE THITHER VOL II</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+\\s*$</td>\n",
       "      <td>102092</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15422</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>ISRAEL POTTER HIS FIFTY YEARS OF EXILE</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+\\.</td>\n",
       "      <td>65516</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15859</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>THE PIAZZA TALES</td>\n",
       "      <td>^\\s*[A-Z,;-]+\\.\\s*$</td>\n",
       "      <td>75491</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21816</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>THE CONFIDENCE MAN HIS MASQUERADE</td>\n",
       "      <td>^CHAPTER\\s+[IVXLCM]+\\.?$</td>\n",
       "      <td>95315</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34970</th>\n",
       "      <td>/Users/ryanlipps/Documents/MSDS/DS5001/data/gu...</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "      <td>PIERRE OR THE AMBIGUITIES</td>\n",
       "      <td>^\\s*[IVXLCM]+\\.\\s*$</td>\n",
       "      <td>155056</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_file_path            author  \\\n",
       "book_id                                                                        \n",
       "105      /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...      AUSTEN, JANE   \n",
       "121      /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...      AUSTEN, JANE   \n",
       "141      /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...      AUSTEN, JANE   \n",
       "158      /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...      AUSTEN, JANE   \n",
       "161      /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...      AUSTEN, JANE   \n",
       "946      /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...      AUSTEN, JANE   \n",
       "1212     /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...      AUSTEN, JANE   \n",
       "1342     /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...      AUSTEN, JANE   \n",
       "1900     /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "2701     /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "4045     /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "8118     /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "10712    /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "13720    /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "13721    /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "15422    /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "15859    /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "21816    /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "34970    /Users/ryanlipps/Documents/MSDS/DS5001/data/gu...  MELVILLE, HERMAN   \n",
       "\n",
       "                                                     title  \\\n",
       "book_id                                                      \n",
       "105                                             PERSUASION   \n",
       "121                                       NORTHANGER ABBEY   \n",
       "141                                         MANSFIELD PARK   \n",
       "158                                                   EMMA   \n",
       "161                                  SENSE AND SENSIBILITY   \n",
       "946                                             LADY SUSAN   \n",
       "1212                               LOVE AND FREINDSHIP SIC   \n",
       "1342                                   PRIDE AND PREJUDICE   \n",
       "1900                     TYPEE A ROMANCE OF THE SOUTH SEAS   \n",
       "2701                                MOBY DICK OR THE WHALE   \n",
       "4045                     OMOO ADVENTURES IN THE SOUTH SEAS   \n",
       "8118     REDBURN HIS FIRST VOYAGE BEING THE SAILOR BOY ...   \n",
       "10712            WHITE JACKET OR THE WORLD ON A MAN OF WAR   \n",
       "13720                     MARDI AND A VOYAGE THITHER VOL I   \n",
       "13721                    MARDI AND A VOYAGE THITHER VOL II   \n",
       "15422               ISRAEL POTTER HIS FIFTY YEARS OF EXILE   \n",
       "15859                                     THE PIAZZA TALES   \n",
       "21816                    THE CONFIDENCE MAN HIS MASQUERADE   \n",
       "34970                            PIERRE OR THE AMBIGUITIES   \n",
       "\n",
       "                              chap_regex  book_len  n_chaps  \n",
       "book_id                                                      \n",
       "105                      ^Chapter\\s+\\d+$     83624       24  \n",
       "121                      ^CHAPTER\\s+\\d+$     77601       31  \n",
       "141                ^CHAPTER\\s+[IVXLCM]+$    160378       48  \n",
       "158          ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$    160926       55  \n",
       "161                      ^CHAPTER\\s+\\d+$    119873       50  \n",
       "946                    ^\\s*[IVXLCM]+\\s*$     23116       41  \n",
       "1212                ^\\s*LETTER .* to .*$     33265       24  \n",
       "1342                     ^Chapter\\s+\\d+$    122126       61  \n",
       "1900                           ^CHAPTER     108021       34  \n",
       "2701     ^(?:ETYMOLOGY|EXTRACTS|CHAPTER)    215504      138  \n",
       "4045       ^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$    102352       82  \n",
       "8118                 ^\\s*[IVXLCM]+\\. .*$    119243       78  \n",
       "10712         ^CHAPTER\\s+[IVXLCM]+\\.\\s*$    143310       92  \n",
       "13720        ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$     96878      104  \n",
       "13721        ^\\s*CHAPTER\\s+[IVXLCM]+\\s*$    102092       91  \n",
       "15422          ^\\s*CHAPTER\\s+[IVXLCM]+\\.     65516       27  \n",
       "15859                ^\\s*[A-Z,;-]+\\.\\s*$     75491        1  \n",
       "21816           ^CHAPTER\\s+[IVXLCM]+\\.?$     95315       90  \n",
       "34970                ^\\s*[IVXLCM]+\\.\\s*$    155056      114  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>('Sir', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Sir</td>\n",
       "      <td>sir</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>('Walter', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Walter</td>\n",
       "      <td>walter</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>('Elliot,', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Elliot,</td>\n",
       "      <td>elliot</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>('of', 'IN')</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>('Kellynch', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Kellynch</td>\n",
       "      <td>kellynch</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         chap_id  para_num  sent_num  token_num            pos_tuple  pos  \\\n",
       "book_id                                                                     \n",
       "105            1         1         0          0       ('Sir', 'NNP')  NNP   \n",
       "105            1         1         0          1    ('Walter', 'NNP')  NNP   \n",
       "105            1         1         0          2   ('Elliot,', 'NNP')  NNP   \n",
       "105            1         1         0          3         ('of', 'IN')   IN   \n",
       "105            1         1         0          4  ('Kellynch', 'NNP')  NNP   \n",
       "\n",
       "        token_str  term_str pos_group  \n",
       "book_id                                \n",
       "105           Sir       sir        NN  \n",
       "105        Walter    walter        NN  \n",
       "105       Elliot,    elliot        NN  \n",
       "105            of        of        IN  \n",
       "105      Kellynch  kellynch        NN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set CORPUS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2058943"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS = CORPUS.reset_index()\n",
    "CORPUS = CORPUS.set_index(['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num']).dropna()\n",
    "CORPUS.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "Show functions\n",
    "\n",
    "### Answer 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BOW(corpus:pd.DataFrame, level:str):\n",
    "    '''\n",
    "    Function to get bag of words from a corpus\n",
    "\n",
    "    Corpus here is loosely defined, as this function will work provided the `level` parameter is in the multi-index of `corpus`\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "    `corpus` - pandas DataFrame of body of work. It must be multi-indexed by an OHCO\n",
    "\n",
    "    `level` - string of OHCO to group by for bags\n",
    "\n",
    "    OUTPUTS:\n",
    "\n",
    "    pandas DataFrame of bag of words grouiped by `level`\n",
    "\n",
    "    EXAMPLE:\n",
    "\n",
    "    `BOW = get_BOW(CORPUS, 'chap_id')`\n",
    "    '''\n",
    "    # Get multi-index from `corpus` df\n",
    "    idx = list(corpus.index.names)\n",
    "\n",
    "    # Check to see that `level` exists in `corpus` OHCO\n",
    "    # Raise error if not\n",
    "    if (level not in idx):\n",
    "        raise KeyError (f'{level} not found in corpus OHCO')\n",
    "\n",
    "    # Split-apply-combine to generate BOW grouped by `level`\n",
    "    return corpus.groupby(idx[:idx.index(level)+1]+['term_str'])\\\n",
    "        .term_str\\\n",
    "        .count()\\\n",
    "        .to_frame('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">105</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          n\n",
       "book_id chap_id term_str   \n",
       "105     1       1         2\n",
       "                15        1\n",
       "                16        1\n",
       "                1760      1\n",
       "                1784      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW = get_BOW(CORPUS, 'chap_id')\n",
    "BOW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TFIDF(bow:pd.DataFrame, tf_type:str):\n",
    "    '''\n",
    "    Function to compute TFIDF for a given bag of words DataFrame\n",
    "\n",
    "    PARAMETERS:\n",
    "\n",
    "    `bow` - pandas DataFrame representation of bag of words\n",
    "    \n",
    "    `tf_type` - string of term frequency type to use. Options are currently:\\n\n",
    "                sum, max, log, raw, double_norm (defaults to k=1), and binary\n",
    "\n",
    "    OUTPUTS:\n",
    "    \n",
    "    tf-idf vectorized DataFrame\n",
    "\n",
    "    EXAMPLE:\n",
    "    `TFIDF = get_TFIDF(BOW, 'max')`\n",
    "    '''\n",
    "\n",
    "    DTCM = bow.n.unstack(fill_value=0)\n",
    "\n",
    "    # Term frequency calculation dictionary\n",
    "    if tf_type == 'sum':\n",
    "        TF = DTCM.T / DTCM.T.sum()\n",
    "\n",
    "    elif tf_type == 'max':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "        \n",
    "    elif tf_type == 'log':\n",
    "        TF = np.log2(1 + DTCM.T)\n",
    "        \n",
    "    elif tf_type == 'raw':\n",
    "        TF = DTCM.T\n",
    "        \n",
    "    elif tf_type == 'double_norm':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "        \n",
    "    elif tf_type == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')\n",
    "        \n",
    "    TF = TF.T\n",
    "    \n",
    "    # Calculate document frequency\n",
    "    DF = DTCM.astype('bool').sum()\n",
    "    \n",
    "    # Calculate number of documents\n",
    "    N = DTCM.shape[0]\n",
    "\n",
    "    # idf calculations\n",
    "    IDF = np.log2(N / DF)\n",
    "    \n",
    "    #TFIDF = TF*IDF\n",
    "\n",
    "    return TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OG WORK\n",
    "# def get_TFIDF(bow:pd.DataFrame, tf_type:str, double_norm=0.5):\n",
    "#     '''\n",
    "#     DOCSTRING GOES HERE\n",
    "#     '''\n",
    "#     # Term frequency calculation dictionary\n",
    "#     tf_dict = {\n",
    "#         'sum':lambda x: x.n / x.n.sum(),\n",
    "#         'max':lambda x: x.n / x.n.max(), \n",
    "#         'log':lambda x: np.log2(1 + x.n), \n",
    "#         'raw':lambda x: x.n, \n",
    "#         'double_norm':lambda x: (double_norm + (1 - double_norm) * (x.n / x.n.max())),\n",
    "#         'binary':lambda x: x.n.astype('bool').astype('int'), \n",
    "#     }\n",
    "\n",
    "#     # Dynamically find levels to drop from bow format\n",
    "#     # This is because we have to group by levels of bow to get TF, but we don't want to repeat those levels in the output\n",
    "#     bow_level_drop = [x for x in range(len(list(BOW.index.names))-1)]\n",
    "\n",
    "#     # Calculate term frequency\n",
    "#     # Assuming bow is indexed by bag-level OHCO, group by bag level\n",
    "#     # Apply parameterized tf computation\n",
    "#     # Cast as frame\n",
    "#     # Drop redundant levels from groupby\n",
    "#     # Rename column\n",
    "#     tf = bow.groupby(list(bow.index.names)[:-1])\\\n",
    "#         .apply(tf_dict.get(tf_type))\\\n",
    "#         .to_frame()\\\n",
    "#         .droplevel(bow_level_drop)\\\n",
    "#         .rename(columns={'n':'tf'})\n",
    "    \n",
    "#     # Calculate document frequency\n",
    "#     df = tf.tf.unstack(fill_value=0)\\\n",
    "#         .astype('bool')\\\n",
    "#         .sum()\n",
    "    \n",
    "#     # Calculate number of documents\n",
    "#     N = tf.groupby(list(bow.index.names)[:-1])\\\n",
    "#         .count()\\\n",
    "#         .shape[0]\n",
    "\n",
    "#     # idf calculations\n",
    "#     idf = np.log2(N / df)\n",
    "    \n",
    "#     return tf #* idf\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VOCAB table from CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = CORPUS\\\n",
    "    .term_str\\\n",
    "        .value_counts()\\\n",
    "        .to_frame('n')\\\n",
    "        .sort_index()\n",
    "VOCAB.index_name = 'term_str'\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)\n",
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9.713722e-07</td>\n",
       "      <td>19.973472</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1.117078e-05</td>\n",
       "      <td>16.449911</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.914117e-06</td>\n",
       "      <td>18.388510</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9.713722e-07</td>\n",
       "      <td>19.973472</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9.713722e-07</td>\n",
       "      <td>19.973472</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           n  n_chars             p          i max_pos\n",
       "term_str                                              \n",
       "0          2        1  9.713722e-07  19.973472      CD\n",
       "1         23        1  1.117078e-05  16.449911      CD\n",
       "10         6        2  2.914117e-06  18.388510      CD\n",
       "100        2        3  9.713722e-07  19.973472      CD\n",
       "1000       2        4  9.713722e-07  19.973472      CD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add TFIDF means to VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB['tfidf_book_max_mean'] = get_TFIDF(get_BOW(CORPUS, 'book_id'), 'max').mean()\n",
    "VOCAB['tfidf_chap_sum_mean'] = get_TFIDF(get_BOW(CORPUS, 'chap_id'), 'sum').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>tfidf_book_max_mean</th>\n",
       "      <th>tfidf_chap_sum_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9.713722e-07</td>\n",
       "      <td>19.973472</td>\n",
       "      <td>CD</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1.117078e-05</td>\n",
       "      <td>16.449911</td>\n",
       "      <td>CD</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.914117e-06</td>\n",
       "      <td>18.388510</td>\n",
       "      <td>CD</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9.713722e-07</td>\n",
       "      <td>19.973472</td>\n",
       "      <td>CD</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9.713722e-07</td>\n",
       "      <td>19.973472</td>\n",
       "      <td>CD</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           n  n_chars             p          i max_pos  tfidf_book_max_mean  \\\n",
       "term_str                                                                      \n",
       "0          2        1  9.713722e-07  19.973472      CD             0.000341   \n",
       "1         23        1  1.117078e-05  16.449911      CD             0.000268   \n",
       "10         6        2  2.914117e-06  18.388510      CD             0.000221   \n",
       "100        2        3  9.713722e-07  19.973472      CD             0.000043   \n",
       "1000       2        4  9.713722e-07  19.973472      CD             0.000040   \n",
       "\n",
       "          tfidf_chap_sum_mean  \n",
       "term_str                       \n",
       "0                    0.000002  \n",
       "1                    0.000047  \n",
       "10                   0.000020  \n",
       "100                  0.000013  \n",
       "1000                 0.000010  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "What are the top 20 words in the corpus by TFIDF mean using the `max` count method and `book` as the bag?\n",
    "\n",
    "### Answer 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>tfidf_book_max_mean</th>\n",
       "      <th>tfidf_chap_sum_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>elinor</th>\n",
       "      <td>623</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>11.690384</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.033840</td>\n",
       "      <td>0.001016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pierre</th>\n",
       "      <td>1526</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>10.397933</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.030911</td>\n",
       "      <td>0.003317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vernon</th>\n",
       "      <td>104</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>14.273033</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.025980</td>\n",
       "      <td>0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marianne</th>\n",
       "      <td>499</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>12.010576</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.021347</td>\n",
       "      <td>0.000843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emma</th>\n",
       "      <td>788</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>11.351421</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>darcy</th>\n",
       "      <td>374</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>12.426578</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.019302</td>\n",
       "      <td>0.000689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reginald</th>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>14.764019</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.018486</td>\n",
       "      <td>0.000678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babbalanja</th>\n",
       "      <td>547</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>11.878075</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.018252</td>\n",
       "      <td>0.001429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catherine</th>\n",
       "      <td>557</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>11.851939</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.018238</td>\n",
       "      <td>0.000874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frederica</th>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>14.803547</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.000440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crawford</th>\n",
       "      <td>493</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>12.028029</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.017749</td>\n",
       "      <td>0.000580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fanny</th>\n",
       "      <td>865</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>11.216916</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.017167</td>\n",
       "      <td>0.000911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elliot</th>\n",
       "      <td>254</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>12.984788</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.017053</td>\n",
       "      <td>0.000338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weston</th>\n",
       "      <td>389</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>12.369846</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>0.000539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media</th>\n",
       "      <td>497</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>12.016370</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.015986</td>\n",
       "      <td>0.001331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>israel</th>\n",
       "      <td>520</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>11.951105</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.015428</td>\n",
       "      <td>0.000887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knightley</th>\n",
       "      <td>356</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>12.497739</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.000455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tilney</th>\n",
       "      <td>196</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>13.358763</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.013815</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elton</th>\n",
       "      <td>320</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>12.651544</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bingley</th>\n",
       "      <td>257</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>12.967848</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.013264</td>\n",
       "      <td>0.000573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               n  n_chars         p          i max_pos  tfidf_book_max_mean  \\\n",
       "term_str                                                                      \n",
       "elinor       623        6  0.000303  11.690384     NNP             0.033840   \n",
       "pierre      1526        6  0.000741  10.397933     NNP             0.030911   \n",
       "vernon       104        6  0.000051  14.273033     NNP             0.025980   \n",
       "marianne     499        8  0.000242  12.010576     NNP             0.021347   \n",
       "emma         788        4  0.000383  11.351421     NNP             0.021164   \n",
       "darcy        374        5  0.000182  12.426578     NNP             0.019302   \n",
       "reginald      74        8  0.000036  14.764019     NNP             0.018486   \n",
       "babbalanja   547       10  0.000266  11.878075     NNP             0.018252   \n",
       "catherine    557        9  0.000271  11.851939     NNP             0.018238   \n",
       "frederica     72        9  0.000035  14.803547     NNP             0.017986   \n",
       "crawford     493        8  0.000239  12.028029     NNP             0.017749   \n",
       "fanny        865        5  0.000420  11.216916     NNP             0.017167   \n",
       "elliot       254        6  0.000123  12.984788     NNP             0.017053   \n",
       "weston       389        6  0.000189  12.369846     NNP             0.016591   \n",
       "media        497        5  0.000241  12.016370     NNP             0.015986   \n",
       "israel       520        6  0.000253  11.951105     NNP             0.015428   \n",
       "knightley    356        9  0.000173  12.497739     NNP             0.015184   \n",
       "tilney       196        6  0.000095  13.358763     NNP             0.013815   \n",
       "elton        320        5  0.000155  12.651544     NNP             0.013648   \n",
       "bingley      257        7  0.000125  12.967848     NNP             0.013264   \n",
       "\n",
       "            tfidf_chap_sum_mean  \n",
       "term_str                         \n",
       "elinor                 0.001016  \n",
       "pierre                 0.003317  \n",
       "vernon                 0.000779  \n",
       "marianne               0.000843  \n",
       "emma                   0.000996  \n",
       "darcy                  0.000689  \n",
       "reginald               0.000678  \n",
       "babbalanja             0.001429  \n",
       "catherine              0.000874  \n",
       "frederica              0.000440  \n",
       "crawford               0.000580  \n",
       "fanny                  0.000911  \n",
       "elliot                 0.000338  \n",
       "weston                 0.000539  \n",
       "media                  0.001331  \n",
       "israel                 0.000887  \n",
       "knightley              0.000455  \n",
       "tilney                 0.000365  \n",
       "elton                  0.000450  \n",
       "bingley                0.000573  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = VOCAB.sort_values('tfidf_book_max_mean', ascending=False).head(20)\n",
    "q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "What are the top 20 words in the corpus by TFIDF mean, if you using the `sum` count method and `paragraph`  `chapter` as the bag? Note, because of the greater number of bags, this will take longer to compute.\n",
    "\n",
    "### Answer 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>tfidf_book_max_mean</th>\n",
       "      <th>tfidf_chap_sum_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>her</th>\n",
       "      <td>16927</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008221</td>\n",
       "      <td>6.926434</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>12059</td>\n",
       "      <td>3</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>7.415650</td>\n",
       "      <td>PRP</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosmopolitan</th>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>14.315261</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.003489</td>\n",
       "      <td>0.003485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pierre</th>\n",
       "      <td>1526</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>10.397933</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.030911</td>\n",
       "      <td>0.003317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communion</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>17.803547</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.003004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>27280</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>6.237916</td>\n",
       "      <td>PRP</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sailors</th>\n",
       "      <td>617</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>11.704346</td>\n",
       "      <td>NNS</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>14347</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>7.165011</td>\n",
       "      <td>PRP</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypothetical</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.388510</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.002437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mr</th>\n",
       "      <td>3388</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>9.247254</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>0.002084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>62954</td>\n",
       "      <td>3</td>\n",
       "      <td>0.030576</td>\n",
       "      <td>5.031462</td>\n",
       "      <td>CC</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidential</th>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>15.218585</td>\n",
       "      <td>JJ</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>109921</td>\n",
       "      <td>3</td>\n",
       "      <td>0.053387</td>\n",
       "      <td>4.227365</td>\n",
       "      <td>DT</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dream</th>\n",
       "      <td>133</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>13.918190</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.001942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boon</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>17.166118</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.001857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrs</th>\n",
       "      <td>2658</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>9.597347</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0.011664</td>\n",
       "      <td>0.001747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elephants</th>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>16.329616</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.001731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whale</th>\n",
       "      <td>1187</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>10.760368</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>912</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>11.140582</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.001696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquaintance</th>\n",
       "      <td>410</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>12.293992</td>\n",
       "      <td>NN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   n  n_chars         p          i max_pos  \\\n",
       "term_str                                                     \n",
       "her            16927        3  0.008221   6.926434    PRP$   \n",
       "she            12059        3  0.005857   7.415650     PRP   \n",
       "cosmopolitan     101       12  0.000049  14.315261      NN   \n",
       "pierre          1526        6  0.000741  10.397933     NNP   \n",
       "communion          9        9  0.000004  17.803547      NN   \n",
       "i              27280        1  0.013250   6.237916     PRP   \n",
       "sailors          617        7  0.000300  11.704346     NNS   \n",
       "you            14347        3  0.006968   7.165011     PRP   \n",
       "hypothetical       3       12  0.000001  19.388510     NNP   \n",
       "mr              3388        2  0.001646   9.247254     NNP   \n",
       "and            62954        3  0.030576   5.031462      CC   \n",
       "confidential      54       12  0.000026  15.218585      JJ   \n",
       "the           109921        3  0.053387   4.227365      DT   \n",
       "dream            133        5  0.000065  13.918190      NN   \n",
       "boon              14        4  0.000007  17.166118      NN   \n",
       "mrs             2658        3  0.001291   9.597347     NNP   \n",
       "elephants         25        9  0.000012  16.329616      NN   \n",
       "whale           1187        5  0.000577  10.760368      NN   \n",
       "thou             912        4  0.000443  11.140582      NN   \n",
       "acquaintance     410       12  0.000199  12.293992      NN   \n",
       "\n",
       "              tfidf_book_max_mean  tfidf_chap_sum_mean  \n",
       "term_str                                                \n",
       "her                      0.000000             0.004327  \n",
       "she                      0.000000             0.004150  \n",
       "cosmopolitan             0.003489             0.003485  \n",
       "pierre                   0.030911             0.003317  \n",
       "communion                0.000107             0.003004  \n",
       "i                        0.000000             0.002771  \n",
       "sailors                  0.002783             0.002668  \n",
       "you                      0.000000             0.002620  \n",
       "hypothetical             0.000104             0.002437  \n",
       "mr                       0.006662             0.002084  \n",
       "and                      0.000000             0.002054  \n",
       "confidential             0.000076             0.002042  \n",
       "the                      0.000000             0.001972  \n",
       "dream                    0.000381             0.001942  \n",
       "boon                     0.000204             0.001857  \n",
       "mrs                      0.011664             0.001747  \n",
       "elephants                0.000274             0.001731  \n",
       "whale                    0.005143             0.001715  \n",
       "thou                     0.003940             0.001696  \n",
       "acquaintance             0.000000             0.001690  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3 = VOCAB.sort_values('tfidf_chap_sum_mean', ascending=False).head(20)\n",
    "q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "Characterize the general difference between the words in Question 3 and those in Question 2 in terms of part-of-speech.\n",
    "\n",
    "### Answer 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_pos\n",
       "NNP    20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2.max_pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_pos\n",
       "NN      8\n",
       "NNP     4\n",
       "PRP     3\n",
       "PRP$    1\n",
       "NNS     1\n",
       "CC      1\n",
       "JJ      1\n",
       "DT      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3.max_pos.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The POS for question 2 are only proper nouns, whereas the POS for question 3 have a mix of different POS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5:\n",
    "Compute mean `TFIDF` for vocabularies conditioned on individual author, using *chapter* as the bag and `max` as the `TF` count method. Among the two authors, whose work has the most significant adjective?\n",
    "\n",
    "### Answer 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create author-specific corpora and vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">105</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>('Sir', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Sir</td>\n",
       "      <td>sir</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('Walter', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Walter</td>\n",
       "      <td>walter</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('Elliot,', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Elliot,</td>\n",
       "      <td>elliot</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('of', 'IN')</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('Kellynch', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Kellynch</td>\n",
       "      <td>kellynch</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1342</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">61</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">18</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>8</th>\n",
       "      <td>('and', 'CC')</td>\n",
       "      <td>CC</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('Prejudice,', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Prejudice,</td>\n",
       "      <td>prejudice</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('by', 'IN')</td>\n",
       "      <td>IN</td>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>('Jane', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Jane</td>\n",
       "      <td>jane</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>('Austen', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Austen</td>\n",
       "      <td>austen</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780752 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         pos_tuple  pos  \\\n",
       "book_id chap_id para_num sent_num token_num                               \n",
       "105     1       1        0        0                 ('Sir', 'NNP')  NNP   \n",
       "                                  1              ('Walter', 'NNP')  NNP   \n",
       "                                  2             ('Elliot,', 'NNP')  NNP   \n",
       "                                  3                   ('of', 'IN')   IN   \n",
       "                                  4            ('Kellynch', 'NNP')  NNP   \n",
       "...                                                            ...  ...   \n",
       "1342    61      18       0        8                  ('and', 'CC')   CC   \n",
       "                                  9          ('Prejudice,', 'NNP')  NNP   \n",
       "                                  10                  ('by', 'IN')   IN   \n",
       "                                  11               ('Jane', 'NNP')  NNP   \n",
       "                                  12             ('Austen', 'NNP')  NNP   \n",
       "\n",
       "                                              token_str   term_str pos_group  \n",
       "book_id chap_id para_num sent_num token_num                                   \n",
       "105     1       1        0        0                 Sir        sir        NN  \n",
       "                                  1              Walter     walter        NN  \n",
       "                                  2             Elliot,     elliot        NN  \n",
       "                                  3                  of         of        IN  \n",
       "                                  4            Kellynch   kellynch        NN  \n",
       "...                                                 ...        ...       ...  \n",
       "1342    61      18       0        8                 and        and        CC  \n",
       "                                  9          Prejudice,  prejudice        NN  \n",
       "                                  10                 by         by        IN  \n",
       "                                  11               Jane       jane        NN  \n",
       "                                  12             Austen     austen        NN  \n",
       "\n",
       "[780752 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_books = list(LIB.query('author == \"AUSTEN, JANE\"').index)\n",
    "AUSTEN = CORPUS.query(f'book_id in {austen_books}')\n",
    "AUSTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1900</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>('THE', 'DT')</td>\n",
       "      <td>DT</td>\n",
       "      <td>THE</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('SEA', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>SEA</td>\n",
       "      <td>sea</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('LONGINGS', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>LONGINGS</td>\n",
       "      <td>longings</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('FOR', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>FOR</td>\n",
       "      <td>for</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('SHORE', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>SHORE</td>\n",
       "      <td>shore</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">34970</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">114</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">24</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>6</th>\n",
       "      <td>('The', 'DT')</td>\n",
       "      <td>DT</td>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('Ambiguities,', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Ambiguities,</td>\n",
       "      <td>ambiguities</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('by', 'IN')</td>\n",
       "      <td>IN</td>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('Herman', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Herman</td>\n",
       "      <td>herman</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('Melville', 'NNP')</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Melville</td>\n",
       "      <td>melville</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1278191 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           pos_tuple  pos  \\\n",
       "book_id chap_id para_num sent_num token_num                                 \n",
       "1900    1       0        0        0                    ('THE', 'DT')   DT   \n",
       "                                  1                   ('SEA', 'NNP')  NNP   \n",
       "                                  2              ('LONGINGS', 'NNP')  NNP   \n",
       "                                  3                   ('FOR', 'NNP')  NNP   \n",
       "                                  4                 ('SHORE', 'NNP')  NNP   \n",
       "...                                                              ...  ...   \n",
       "34970   114     24       0        6                    ('The', 'DT')   DT   \n",
       "                                  7          ('Ambiguities,', 'NNP')  NNP   \n",
       "                                  8                     ('by', 'IN')   IN   \n",
       "                                  9                ('Herman', 'NNP')  NNP   \n",
       "                                  10             ('Melville', 'NNP')  NNP   \n",
       "\n",
       "                                                token_str     term_str  \\\n",
       "book_id chap_id para_num sent_num token_num                              \n",
       "1900    1       0        0        0                   THE          the   \n",
       "                                  1                   SEA          sea   \n",
       "                                  2              LONGINGS     longings   \n",
       "                                  3                   FOR          for   \n",
       "                                  4                 SHORE        shore   \n",
       "...                                                   ...          ...   \n",
       "34970   114     24       0        6                   The          the   \n",
       "                                  7          Ambiguities,  ambiguities   \n",
       "                                  8                    by           by   \n",
       "                                  9                Herman       herman   \n",
       "                                  10             Melville     melville   \n",
       "\n",
       "                                            pos_group  \n",
       "book_id chap_id para_num sent_num token_num            \n",
       "1900    1       0        0        0                DT  \n",
       "                                  1                NN  \n",
       "                                  2                NN  \n",
       "                                  3                NN  \n",
       "                                  4                NN  \n",
       "...                                               ...  \n",
       "34970   114     24       0        6                DT  \n",
       "                                  7                NN  \n",
       "                                  8                IN  \n",
       "                                  9                NN  \n",
       "                                  10               NN  \n",
       "\n",
       "[1278191 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melville = list(LIB.query('author == \"MELVILLE, HERMAN\"').index)\n",
    "MELVILLE = CORPUS.query(f'book_id in {melville}')\n",
    "MELVILLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUSTEN_VOCAB = AUSTEN\\\n",
    "    .term_str\\\n",
    "        .value_counts()\\\n",
    "        .to_frame('n')\\\n",
    "        .sort_index()\n",
    "AUSTEN_VOCAB.index_name = 'term_str'\n",
    "AUSTEN_VOCAB['n_chars'] = AUSTEN_VOCAB.index.str.len()\n",
    "AUSTEN_VOCAB['p'] = AUSTEN_VOCAB.n / AUSTEN_VOCAB.n.sum()\n",
    "AUSTEN_VOCAB['i'] = -np.log2(AUSTEN_VOCAB.p)\n",
    "AUSTEN_VOCAB['max_pos'] = AUSTEN[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MELVILLE_VOCAB = MELVILLE\\\n",
    "    .term_str\\\n",
    "        .value_counts()\\\n",
    "        .to_frame('n')\\\n",
    "        .sort_index()\n",
    "MELVILLE_VOCAB.index_name = 'term_str'\n",
    "MELVILLE_VOCAB['n_chars'] = MELVILLE_VOCAB.index.str.len()\n",
    "MELVILLE_VOCAB['p'] = MELVILLE_VOCAB.n / MELVILLE_VOCAB.n.sum()\n",
    "MELVILLE_VOCAB['i'] = -np.log2(MELVILLE_VOCAB.p)\n",
    "MELVILLE_VOCAB['max_pos'] = MELVILLE[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute chapter-bag max mean TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUSTEN_VOCAB['tfidf_chap_max_mean'] = get_TFIDF(get_BOW(AUSTEN, 'chap_id'), 'max').mean()\n",
    "MELVILLE_VOCAB['tfidf_chap_max_mean'] = get_TFIDF(get_BOW(MELVILLE, 'chap_id'), 'max').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return most significant adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>tfidf_chap_max_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>778</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>9.970878</td>\n",
       "      <td>JJ</td>\n",
       "      <td>0.013167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            n  n_chars         p         i max_pos  tfidf_chap_max_mean\n",
       "term_str                                                               \n",
       "sure      778        4  0.000996  9.970878      JJ             0.013167"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUSTEN_VOCAB.sort_values('tfidf_chap_max_mean', ascending=False).query('max_pos==\"JJ\"').head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>tfidf_chap_max_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>594</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>11.071353</td>\n",
       "      <td>JJ</td>\n",
       "      <td>0.028653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            n  n_chars         p          i max_pos  tfidf_chap_max_mean\n",
       "term_str                                                                \n",
       "thy       594        3  0.000465  11.071353      JJ             0.028653"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MELVILLE_VOCAB.sort_values('tfidf_chap_max_mean', ascending=False).query('max_pos==\"JJ\"').head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melville has the most significant adjective in 'thy'.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-6001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235.517px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
